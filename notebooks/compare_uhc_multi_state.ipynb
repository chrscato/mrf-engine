{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Multi-State UHC Rates Comparison\n",
        "\n",
        "Comparing all UHC state/plan extractions to identify:\n",
        "1. Duplication patterns across states/plans\n",
        "2. Subset relationships (which files contain others)\n",
        "3. Master file identification (largest/most comprehensive)\n",
        "4. Unique data per file\n",
        "\n",
        "**Files being compared:**\n",
        "- IL Choice Plus (Dec 19, 2025)\n",
        "- IL Core POS (Dec 27, 2025)\n",
        "- NC Choice Plus (Jan 2, 2026)\n",
        "- NY Choice EPO (Dec 28, 2025)\n",
        "- NY Choice Plus (Dec 29, 2025)\n",
        "- GA Charter HMO (Dec 30, 2025)\n",
        "- GA Choice HMO (Jan 1, 2026)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üìÅ File Verification:\n",
            "================================================================================\n",
            "‚úÖ IL Choice Plus        1.81 GB\n",
            "‚úÖ IL Core POS           1.70 GB\n",
            "‚úÖ NC Choice Plus        1.76 GB\n",
            "‚úÖ NY Choice EPO         1.75 GB\n",
            "‚úÖ NY Choice Plus        1.75 GB\n",
            "‚úÖ GA Charter HMO        1.61 GB\n",
            "‚úÖ GA Choice HMO         1.75 GB\n",
            "\n",
            "üìä Total files to compare: 7\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import pyarrow.parquet as pq\n",
        "from pathlib import Path\n",
        "from collections import defaultdict\n",
        "\n",
        "# Define all UHC files to compare\n",
        "uhc_files = {\n",
        "    \"IL Choice Plus\": \"output/il_uhc/rates_il_uhc_choice_plus_20251219_194236.parquet\",\n",
        "    \"IL Core POS\": \"output/il_uhc/rates_il_uhc_core_pos_20251227_121655.parquet\",\n",
        "    \"NC Choice Plus\": \"output/nc_uhc/rates_nc_uhc_choice_plus_20260102_112128.parquet\",\n",
        "    \"NY Choice EPO\": \"output/ny_uhc/rates_ny_uhc_choice_epo_20251228_112232.parquet\",\n",
        "    \"NY Choice Plus\": \"output/ny_uhc/rates_ny_uhc_choice_plus_20251229_160021.parquet\",\n",
        "    \"GA Charter HMO\": \"output/ga_uhc/rates_ga_uhc_charter_hmo_20251230_113902.parquet\",\n",
        "    \"GA Choice HMO\": \"output/ga_uhc/rates_ga_uhc_choice_hmo_20260101_182048.parquet\",\n",
        "}\n",
        "\n",
        "# Verify files exist and get sizes\n",
        "print(\"üìÅ File Verification:\")\n",
        "print(\"=\"*80)\n",
        "existing_files = {}\n",
        "for name, path in uhc_files.items():\n",
        "    file_path = Path(path)\n",
        "    if file_path.exists():\n",
        "        size_gb = file_path.stat().st_size / (1024**3)\n",
        "        existing_files[name] = path\n",
        "        print(f\"‚úÖ {name:20s} {size_gb:5.2f} GB\")\n",
        "    else:\n",
        "        print(f\"‚ùå {name:20s} NOT FOUND: {path}\")\n",
        "\n",
        "print(f\"\\nüìä Total files to compare: {len(existing_files)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "================================================================================\n",
            "EXTRACTING UNIQUE KEYS FROM ALL FILES (ONE AT A TIME)\n",
            "================================================================================\n",
            "Results will be saved to disk to avoid memory issues\n",
            "\n",
            "\n",
            "IL Choice Plus:\n",
            "  Rows: 220,863,665\n",
            "  Row groups: 11044\n",
            "\n",
            "üì• Streaming IL Choice Plus (processing ALL 11044 row groups)...\n",
            "  Processed 100/11044 row groups...\n",
            "  Processed 200/11044 row groups...\n",
            "  Processed 300/11044 row groups...\n",
            "  Processed 400/11044 row groups...\n",
            "  Processed 500/11044 row groups...\n",
            "  Processed 600/11044 row groups...\n",
            "  Processed 700/11044 row groups...\n",
            "  Processed 800/11044 row groups...\n",
            "  Processed 900/11044 row groups...\n",
            "  Processed 1000/11044 row groups...\n",
            "  Processed 1100/11044 row groups...\n",
            "  Processed 1200/11044 row groups...\n",
            "  Processed 1300/11044 row groups...\n",
            "  Processed 1400/11044 row groups...\n",
            "  Processed 1500/11044 row groups...\n",
            "  Processed 1600/11044 row groups...\n",
            "  Processed 1700/11044 row groups...\n",
            "  Processed 1800/11044 row groups...\n",
            "  Processed 1900/11044 row groups...\n",
            "  Processed 2000/11044 row groups...\n",
            "  Processed 2100/11044 row groups...\n",
            "  Processed 2200/11044 row groups...\n",
            "  Processed 2300/11044 row groups...\n",
            "  Processed 2400/11044 row groups...\n",
            "  Processed 2500/11044 row groups...\n",
            "  Processed 2600/11044 row groups...\n",
            "  Processed 2700/11044 row groups...\n",
            "  Processed 2800/11044 row groups...\n",
            "  Processed 2900/11044 row groups...\n",
            "  Processed 3000/11044 row groups...\n",
            "  Processed 3100/11044 row groups...\n",
            "  Processed 3200/11044 row groups...\n",
            "  Processed 3300/11044 row groups...\n",
            "  Processed 3400/11044 row groups...\n",
            "  Processed 3500/11044 row groups...\n",
            "  Processed 3600/11044 row groups...\n",
            "  Processed 3700/11044 row groups...\n",
            "  Processed 3800/11044 row groups...\n",
            "  Processed 3900/11044 row groups...\n",
            "  Processed 4000/11044 row groups...\n",
            "  Processed 4100/11044 row groups...\n",
            "  Processed 4200/11044 row groups...\n",
            "  Processed 4300/11044 row groups...\n",
            "  Processed 4400/11044 row groups...\n",
            "  Processed 4500/11044 row groups...\n",
            "  Processed 4600/11044 row groups...\n",
            "  Processed 4700/11044 row groups...\n",
            "  Processed 4800/11044 row groups...\n",
            "  Processed 4900/11044 row groups...\n",
            "  Processed 5000/11044 row groups...\n",
            "  Processed 5100/11044 row groups...\n",
            "  Processed 5200/11044 row groups...\n",
            "  Processed 5300/11044 row groups...\n",
            "  Processed 5400/11044 row groups...\n",
            "  Processed 5500/11044 row groups...\n",
            "  Processed 5600/11044 row groups...\n",
            "  Processed 5700/11044 row groups...\n",
            "  Processed 5800/11044 row groups...\n",
            "  Processed 5900/11044 row groups...\n",
            "  Processed 6000/11044 row groups...\n",
            "  Processed 6100/11044 row groups...\n",
            "  Processed 6200/11044 row groups...\n",
            "  Processed 6300/11044 row groups...\n",
            "  Processed 6400/11044 row groups...\n",
            "  Processed 6500/11044 row groups...\n",
            "  Processed 6600/11044 row groups...\n",
            "  Processed 6700/11044 row groups...\n",
            "  Processed 6800/11044 row groups...\n",
            "  Processed 6900/11044 row groups...\n",
            "  Processed 7000/11044 row groups...\n",
            "  Processed 7100/11044 row groups...\n",
            "  Processed 7200/11044 row groups...\n",
            "  Processed 7300/11044 row groups...\n",
            "  Processed 7400/11044 row groups...\n",
            "  Processed 7500/11044 row groups...\n",
            "  Processed 7600/11044 row groups...\n",
            "  Processed 7700/11044 row groups...\n",
            "  Processed 7800/11044 row groups...\n",
            "  Processed 7900/11044 row groups...\n",
            "  Processed 8000/11044 row groups...\n",
            "  Processed 8100/11044 row groups...\n",
            "  Processed 8200/11044 row groups...\n",
            "  Processed 8300/11044 row groups...\n",
            "  Processed 8400/11044 row groups...\n",
            "  Processed 8500/11044 row groups...\n",
            "  Processed 8600/11044 row groups...\n",
            "  Processed 8700/11044 row groups...\n",
            "  Processed 8800/11044 row groups...\n",
            "  Processed 8900/11044 row groups...\n",
            "  Processed 9000/11044 row groups...\n",
            "  Processed 9100/11044 row groups...\n",
            "  Processed 9200/11044 row groups...\n",
            "  Processed 9300/11044 row groups...\n",
            "  Processed 9400/11044 row groups...\n",
            "  Processed 9500/11044 row groups...\n",
            "  Processed 9600/11044 row groups...\n",
            "  Processed 9700/11044 row groups...\n",
            "  Processed 9800/11044 row groups...\n",
            "  Processed 9900/11044 row groups...\n",
            "  Processed 10000/11044 row groups...\n",
            "  Processed 10100/11044 row groups...\n",
            "  Processed 10200/11044 row groups...\n",
            "  Processed 10300/11044 row groups...\n",
            "  Processed 10400/11044 row groups...\n",
            "  Processed 10500/11044 row groups...\n",
            "  Processed 10600/11044 row groups...\n",
            "  Processed 10700/11044 row groups...\n",
            "  Processed 10800/11044 row groups...\n",
            "  Processed 10900/11044 row groups...\n",
            "  Processed 11000/11044 row groups...\n",
            "  Processed 11044/11044 row groups...\n",
            "  üíæ Saving to cache...\n",
            "\n",
            "IL Core POS:\n",
            "  Rows: 208,410,393\n",
            "  Row groups: 10421\n",
            "\n",
            "üì• Streaming IL Core POS (processing ALL 10421 row groups)...\n",
            "  Processed 100/10421 row groups...\n",
            "  Processed 200/10421 row groups...\n",
            "  Processed 300/10421 row groups...\n",
            "  Processed 400/10421 row groups...\n",
            "  Processed 500/10421 row groups...\n",
            "  Processed 600/10421 row groups...\n",
            "  Processed 700/10421 row groups...\n",
            "  Processed 800/10421 row groups...\n",
            "  Processed 900/10421 row groups...\n",
            "  Processed 1000/10421 row groups...\n",
            "  Processed 1100/10421 row groups...\n",
            "  Processed 1200/10421 row groups...\n",
            "  Processed 1300/10421 row groups...\n",
            "  Processed 1400/10421 row groups...\n",
            "  Processed 1500/10421 row groups...\n",
            "  Processed 1600/10421 row groups...\n",
            "  Processed 1700/10421 row groups...\n",
            "  Processed 1800/10421 row groups...\n",
            "  Processed 1900/10421 row groups...\n",
            "  Processed 2000/10421 row groups...\n",
            "  Processed 2100/10421 row groups...\n",
            "  Processed 2200/10421 row groups...\n",
            "  Processed 2300/10421 row groups...\n",
            "  Processed 2400/10421 row groups...\n",
            "  Processed 2500/10421 row groups...\n",
            "  Processed 2600/10421 row groups...\n",
            "  Processed 2700/10421 row groups...\n",
            "  Processed 2800/10421 row groups...\n",
            "  Processed 2900/10421 row groups...\n",
            "  Processed 3000/10421 row groups...\n",
            "  Processed 3100/10421 row groups...\n",
            "  Processed 3200/10421 row groups...\n",
            "  Processed 3300/10421 row groups...\n",
            "  Processed 3400/10421 row groups...\n",
            "  Processed 3500/10421 row groups...\n",
            "  Processed 3600/10421 row groups...\n",
            "  Processed 3700/10421 row groups...\n",
            "  Processed 3800/10421 row groups...\n",
            "  Processed 3900/10421 row groups...\n",
            "  Processed 4000/10421 row groups...\n",
            "  Processed 4100/10421 row groups...\n",
            "  Processed 4200/10421 row groups...\n",
            "  Processed 4300/10421 row groups...\n",
            "  Processed 4400/10421 row groups...\n",
            "  Processed 4500/10421 row groups...\n",
            "  Processed 4600/10421 row groups...\n",
            "  Processed 4700/10421 row groups...\n",
            "  Processed 4800/10421 row groups...\n",
            "  Processed 4900/10421 row groups...\n",
            "  Processed 5000/10421 row groups...\n",
            "  Processed 5100/10421 row groups...\n",
            "  Processed 5200/10421 row groups...\n",
            "  Processed 5300/10421 row groups...\n",
            "  Processed 5400/10421 row groups...\n",
            "  Processed 5500/10421 row groups...\n",
            "  Processed 5600/10421 row groups...\n",
            "  Processed 5700/10421 row groups...\n",
            "  Processed 5800/10421 row groups...\n",
            "  Processed 5900/10421 row groups...\n",
            "  Processed 6000/10421 row groups...\n",
            "  Processed 6100/10421 row groups...\n",
            "  Processed 6200/10421 row groups...\n",
            "  Processed 6300/10421 row groups...\n",
            "  Processed 6400/10421 row groups...\n",
            "  Processed 6500/10421 row groups...\n",
            "  Processed 6600/10421 row groups...\n",
            "  Processed 6700/10421 row groups...\n",
            "  Processed 6800/10421 row groups...\n",
            "  Processed 6900/10421 row groups...\n",
            "  Processed 7000/10421 row groups...\n",
            "  Processed 7100/10421 row groups...\n",
            "  Processed 7200/10421 row groups...\n",
            "  Processed 7300/10421 row groups...\n",
            "  Processed 7400/10421 row groups...\n",
            "  Processed 7500/10421 row groups...\n",
            "  Processed 7600/10421 row groups...\n",
            "  Processed 7700/10421 row groups...\n",
            "  Processed 7800/10421 row groups...\n",
            "  Processed 7900/10421 row groups...\n",
            "  Processed 8000/10421 row groups...\n",
            "  Processed 8100/10421 row groups...\n",
            "  Processed 8200/10421 row groups...\n",
            "  Processed 8300/10421 row groups...\n",
            "  Processed 8400/10421 row groups...\n",
            "  Processed 8500/10421 row groups...\n",
            "  Processed 8600/10421 row groups...\n",
            "  Processed 8700/10421 row groups...\n",
            "  Processed 8800/10421 row groups...\n",
            "  Processed 8900/10421 row groups...\n",
            "  Processed 9000/10421 row groups...\n",
            "  Processed 9100/10421 row groups...\n",
            "  Processed 9200/10421 row groups...\n",
            "  Processed 9300/10421 row groups...\n",
            "  Processed 9400/10421 row groups...\n",
            "  Processed 9500/10421 row groups...\n",
            "  Processed 9600/10421 row groups...\n",
            "  Processed 9700/10421 row groups...\n",
            "  Processed 9800/10421 row groups...\n",
            "  Processed 9900/10421 row groups...\n",
            "  Processed 10000/10421 row groups...\n",
            "  Processed 10100/10421 row groups...\n",
            "  Processed 10200/10421 row groups...\n",
            "  Processed 10300/10421 row groups...\n",
            "  Processed 10400/10421 row groups...\n",
            "  Processed 10421/10421 row groups...\n",
            "  üíæ Saving to cache...\n",
            "\n",
            "NC Choice Plus:\n",
            "  Rows: 222,055,672\n",
            "  Row groups: 4442\n",
            "\n",
            "üì• Streaming NC Choice Plus (processing ALL 4442 row groups)...\n",
            "  Processed 100/4442 row groups...\n",
            "  Processed 200/4442 row groups...\n",
            "  Processed 300/4442 row groups...\n",
            "  Processed 400/4442 row groups...\n",
            "  Processed 500/4442 row groups...\n",
            "  Processed 600/4442 row groups...\n",
            "  Processed 700/4442 row groups...\n",
            "  Processed 800/4442 row groups...\n",
            "  Processed 900/4442 row groups...\n",
            "  Processed 1000/4442 row groups...\n",
            "  Processed 1100/4442 row groups...\n",
            "  Processed 1200/4442 row groups...\n",
            "  Processed 1300/4442 row groups...\n",
            "  Processed 1400/4442 row groups...\n",
            "  Processed 1500/4442 row groups...\n",
            "  Processed 1600/4442 row groups...\n",
            "  Processed 1700/4442 row groups...\n",
            "  Processed 1800/4442 row groups...\n",
            "  Processed 1900/4442 row groups...\n",
            "  Processed 2000/4442 row groups...\n",
            "  Processed 2100/4442 row groups...\n",
            "  Processed 2200/4442 row groups...\n",
            "  Processed 2300/4442 row groups...\n",
            "  Processed 2400/4442 row groups...\n",
            "  Processed 2500/4442 row groups...\n",
            "  Processed 2600/4442 row groups...\n",
            "  Processed 2700/4442 row groups...\n",
            "  Processed 2800/4442 row groups...\n",
            "  Processed 2900/4442 row groups...\n",
            "  Processed 3000/4442 row groups...\n",
            "  Processed 3100/4442 row groups...\n",
            "  Processed 3200/4442 row groups...\n",
            "  Processed 3300/4442 row groups...\n",
            "  Processed 3400/4442 row groups...\n",
            "  Processed 3500/4442 row groups...\n",
            "  Processed 3600/4442 row groups...\n",
            "  Processed 3700/4442 row groups...\n",
            "  Processed 3800/4442 row groups...\n",
            "  Processed 3900/4442 row groups...\n",
            "  Processed 4000/4442 row groups...\n",
            "  Processed 4100/4442 row groups...\n",
            "  Processed 4200/4442 row groups...\n",
            "  Processed 4300/4442 row groups...\n",
            "  Processed 4400/4442 row groups...\n",
            "  Processed 4442/4442 row groups...\n",
            "  üíæ Saving to cache...\n",
            "\n",
            "NY Choice EPO:\n",
            "  Rows: 220,990,878\n",
            "  Row groups: 4420\n",
            "\n",
            "üì• Streaming NY Choice EPO (processing ALL 4420 row groups)...\n",
            "  Processed 100/4420 row groups...\n",
            "  Processed 200/4420 row groups...\n",
            "  Processed 300/4420 row groups...\n",
            "  Processed 400/4420 row groups...\n",
            "  Processed 500/4420 row groups...\n",
            "  Processed 600/4420 row groups...\n",
            "  Processed 700/4420 row groups...\n",
            "  Processed 800/4420 row groups...\n",
            "  Processed 900/4420 row groups...\n",
            "  Processed 1000/4420 row groups...\n",
            "  Processed 1100/4420 row groups...\n",
            "  Processed 1200/4420 row groups...\n",
            "  Processed 1300/4420 row groups...\n",
            "  Processed 1400/4420 row groups...\n",
            "  Processed 1500/4420 row groups...\n",
            "  Processed 1600/4420 row groups...\n",
            "  Processed 1700/4420 row groups...\n",
            "  Processed 1800/4420 row groups...\n",
            "  Processed 1900/4420 row groups...\n",
            "  Processed 2000/4420 row groups...\n",
            "  Processed 2100/4420 row groups...\n",
            "  Processed 2200/4420 row groups...\n",
            "  Processed 2300/4420 row groups...\n",
            "  Processed 2400/4420 row groups...\n",
            "  Processed 2500/4420 row groups...\n",
            "  Processed 2600/4420 row groups...\n",
            "  Processed 2700/4420 row groups...\n",
            "  Processed 2800/4420 row groups...\n",
            "  Processed 2900/4420 row groups...\n",
            "  Processed 3000/4420 row groups...\n",
            "  Processed 3100/4420 row groups...\n",
            "  Processed 3200/4420 row groups...\n",
            "  Processed 3300/4420 row groups...\n",
            "  Processed 3400/4420 row groups...\n",
            "  Processed 3500/4420 row groups...\n",
            "  Processed 3600/4420 row groups...\n",
            "  Processed 3700/4420 row groups...\n",
            "  Processed 3800/4420 row groups...\n",
            "  Processed 3900/4420 row groups...\n",
            "  Processed 4000/4420 row groups...\n",
            "  Processed 4100/4420 row groups...\n",
            "  Processed 4200/4420 row groups...\n",
            "  Processed 4300/4420 row groups...\n",
            "  Processed 4400/4420 row groups...\n",
            "  Processed 4420/4420 row groups...\n",
            "  üíæ Saving to cache...\n",
            "\n",
            "NY Choice Plus:\n",
            "  Rows: 220,906,617\n",
            "  Row groups: 4419\n",
            "\n",
            "üì• Streaming NY Choice Plus (processing ALL 4419 row groups)...\n",
            "  Processed 100/4419 row groups...\n",
            "  Processed 200/4419 row groups...\n",
            "  Processed 300/4419 row groups...\n",
            "  Processed 400/4419 row groups...\n",
            "  Processed 500/4419 row groups...\n",
            "  Processed 600/4419 row groups...\n",
            "  Processed 700/4419 row groups...\n",
            "  Processed 800/4419 row groups...\n",
            "  Processed 900/4419 row groups...\n",
            "  Processed 1000/4419 row groups...\n",
            "  Processed 1100/4419 row groups...\n",
            "  Processed 1200/4419 row groups...\n",
            "  Processed 1300/4419 row groups...\n",
            "  Processed 1400/4419 row groups...\n",
            "  Processed 1500/4419 row groups...\n",
            "  Processed 1600/4419 row groups...\n",
            "  Processed 1700/4419 row groups...\n",
            "  Processed 1800/4419 row groups...\n",
            "  Processed 1900/4419 row groups...\n",
            "  Processed 2000/4419 row groups...\n",
            "  Processed 2100/4419 row groups...\n",
            "  Processed 2200/4419 row groups...\n",
            "  Processed 2300/4419 row groups...\n",
            "  Processed 2400/4419 row groups...\n",
            "  Processed 2500/4419 row groups...\n",
            "  Processed 2600/4419 row groups...\n",
            "  Processed 2700/4419 row groups...\n",
            "  Processed 2800/4419 row groups...\n",
            "  Processed 2900/4419 row groups...\n",
            "  Processed 3000/4419 row groups...\n",
            "  Processed 3100/4419 row groups...\n",
            "  Processed 3200/4419 row groups...\n",
            "  Processed 3300/4419 row groups...\n",
            "  Processed 3400/4419 row groups...\n",
            "  Processed 3500/4419 row groups...\n",
            "  Processed 3600/4419 row groups...\n",
            "  Processed 3700/4419 row groups...\n",
            "  Processed 3800/4419 row groups...\n",
            "  Processed 3900/4419 row groups...\n",
            "  Processed 4000/4419 row groups...\n",
            "  Processed 4100/4419 row groups...\n",
            "  Processed 4200/4419 row groups...\n",
            "  Processed 4300/4419 row groups...\n",
            "  Processed 4400/4419 row groups...\n",
            "  Processed 4419/4419 row groups...\n",
            "  üíæ Saving to cache...\n",
            "\n",
            "GA Charter HMO:\n",
            "  Rows: 206,050,747\n",
            "  Row groups: 4122\n",
            "\n",
            "üì• Streaming GA Charter HMO (processing ALL 4122 row groups)...\n",
            "  Processed 100/4122 row groups...\n",
            "  Processed 200/4122 row groups...\n",
            "  Processed 300/4122 row groups...\n",
            "  Processed 400/4122 row groups...\n",
            "  Processed 500/4122 row groups...\n",
            "  Processed 600/4122 row groups...\n",
            "  Processed 700/4122 row groups...\n",
            "  Processed 800/4122 row groups...\n",
            "  Processed 900/4122 row groups...\n",
            "  Processed 1000/4122 row groups...\n",
            "  Processed 1100/4122 row groups...\n",
            "  Processed 1200/4122 row groups...\n",
            "  Processed 1300/4122 row groups...\n",
            "  Processed 1400/4122 row groups...\n",
            "  Processed 1500/4122 row groups...\n",
            "  Processed 1600/4122 row groups...\n",
            "  Processed 1700/4122 row groups...\n",
            "  Processed 1800/4122 row groups...\n",
            "  Processed 1900/4122 row groups...\n",
            "  Processed 2000/4122 row groups...\n",
            "  Processed 2100/4122 row groups...\n",
            "  Processed 2200/4122 row groups...\n",
            "  Processed 2300/4122 row groups...\n",
            "  Processed 2400/4122 row groups...\n",
            "  Processed 2500/4122 row groups...\n",
            "  Processed 2600/4122 row groups...\n",
            "  Processed 2700/4122 row groups...\n",
            "  Processed 2800/4122 row groups...\n",
            "  Processed 2900/4122 row groups...\n",
            "  Processed 3000/4122 row groups...\n",
            "  Processed 3100/4122 row groups...\n",
            "  Processed 3200/4122 row groups...\n",
            "  Processed 3300/4122 row groups...\n",
            "  Processed 3400/4122 row groups...\n",
            "  Processed 3500/4122 row groups...\n",
            "  Processed 3600/4122 row groups...\n",
            "  Processed 3700/4122 row groups...\n",
            "  Processed 3800/4122 row groups...\n",
            "  Processed 3900/4122 row groups...\n",
            "  Processed 4000/4122 row groups...\n",
            "  Processed 4100/4122 row groups...\n",
            "  Processed 4122/4122 row groups...\n",
            "  üíæ Saving to cache...\n",
            "\n",
            "GA Choice HMO:\n",
            "  Rows: 221,923,092\n",
            "  Row groups: 4439\n",
            "\n",
            "üì• Streaming GA Choice HMO (processing ALL 4439 row groups)...\n",
            "  Processed 100/4439 row groups...\n",
            "  Processed 200/4439 row groups...\n",
            "  Processed 300/4439 row groups...\n",
            "  Processed 400/4439 row groups...\n",
            "  Processed 500/4439 row groups...\n",
            "  Processed 600/4439 row groups...\n",
            "  Processed 700/4439 row groups...\n",
            "  Processed 800/4439 row groups...\n",
            "  Processed 900/4439 row groups...\n",
            "  Processed 1000/4439 row groups...\n",
            "  Processed 1100/4439 row groups...\n",
            "  Processed 1200/4439 row groups...\n",
            "  Processed 1300/4439 row groups...\n",
            "  Processed 1400/4439 row groups...\n",
            "  Processed 1500/4439 row groups...\n",
            "  Processed 1600/4439 row groups...\n",
            "  Processed 1700/4439 row groups...\n",
            "  Processed 1800/4439 row groups...\n",
            "  Processed 1900/4439 row groups...\n",
            "  Processed 2000/4439 row groups...\n",
            "  Processed 2100/4439 row groups...\n",
            "  Processed 2200/4439 row groups...\n",
            "  Processed 2300/4439 row groups...\n",
            "  Processed 2400/4439 row groups...\n",
            "  Processed 2500/4439 row groups...\n",
            "  Processed 2600/4439 row groups...\n",
            "  Processed 2700/4439 row groups...\n",
            "  Processed 2800/4439 row groups...\n",
            "  Processed 2900/4439 row groups...\n",
            "  Processed 3000/4439 row groups...\n",
            "  Processed 3100/4439 row groups...\n",
            "  Processed 3200/4439 row groups...\n",
            "  Processed 3300/4439 row groups...\n",
            "  Processed 3400/4439 row groups...\n",
            "  Processed 3500/4439 row groups...\n",
            "  Processed 3600/4439 row groups...\n",
            "  Processed 3700/4439 row groups...\n",
            "  Processed 3800/4439 row groups...\n",
            "  Processed 3900/4439 row groups...\n",
            "  Processed 4000/4439 row groups...\n",
            "  Processed 4100/4439 row groups...\n",
            "  Processed 4200/4439 row groups...\n",
            "  Processed 4300/4439 row groups...\n",
            "  Processed 4400/4439 row groups...\n",
            "  Processed 4439/4439 row groups...\n",
            "  üíæ Saving to cache...\n",
            "\n",
            "================================================================================\n",
            "‚úÖ EXTRACTION COMPLETE - All results cached to disk\n",
            "================================================================================\n",
            "IL Choice Plus      : 26,434 providers, 118,839,760 rate keys, 3,753 codes\n",
            "IL Core POS         : 25,025 providers, 112,288,723 rate keys, 3,754 codes\n",
            "NC Choice Plus      : 26,437 providers, 119,461,113 rate keys, 3,768 codes\n",
            "NY Choice EPO       : 26,470 providers, 118,958,754 rate keys, 3,754 codes\n",
            "NY Choice Plus      : 26,437 providers, 118,862,217 rate keys, 3,754 codes\n",
            "GA Charter HMO      : 24,661 providers, 110,864,888 rate keys, 3,754 codes\n",
            "GA Choice HMO       : 26,358 providers, 119,357,919 rate keys, 3,768 codes\n"
          ]
        }
      ],
      "source": [
        "# Memory-efficient streaming: Extract unique keys from ONE file at a time\n",
        "# Save results to disk to avoid memory issues with multiple large sets\n",
        "\n",
        "import pickle\n",
        "from pathlib import Path\n",
        "\n",
        "def extract_unique_keys(pq_file, label):\n",
        "    \"\"\"Extract unique provider IDs, rate keys, and billing codes from ENTIRE file via streaming\"\"\"\n",
        "    provider_ids = set()\n",
        "    rate_keys = set()\n",
        "    billing_codes = set()\n",
        "    \n",
        "    total_groups = pq_file.num_row_groups\n",
        "    columns = ['provider_reference_id', 'billing_code', 'billing_code_type', 'negotiated_rate']\n",
        "    \n",
        "    print(f\"\\nüì• Streaming {label} (processing ALL {total_groups} row groups)...\")\n",
        "    for i in range(total_groups):\n",
        "        # Read one row group at a time (memory-efficient)\n",
        "        table = pq_file.read_row_group(i, columns=columns)\n",
        "        df_chunk = table.to_pandas()\n",
        "        \n",
        "        # Extract unique values (updates sets, not storing full dataframe)\n",
        "        provider_ids.update(df_chunk['provider_reference_id'].unique())\n",
        "        billing_codes.update(df_chunk['billing_code'].unique())\n",
        "        \n",
        "        # Create rate keys (provider|code|rate|type)\n",
        "        rate_key_series = (\n",
        "            df_chunk['provider_reference_id'].astype(str) + '|' +\n",
        "            df_chunk['billing_code'].astype(str) + '|' +\n",
        "            df_chunk['negotiated_rate'].astype(str) + '|' +\n",
        "            df_chunk['billing_code_type'].astype(str)\n",
        "        )\n",
        "        rate_keys.update(rate_key_series.unique())\n",
        "        \n",
        "        if (i + 1) % 100 == 0 or (i + 1) == total_groups:\n",
        "            print(f\"  Processed {i+1}/{total_groups} row groups...\")\n",
        "    \n",
        "    return provider_ids, rate_keys, billing_codes\n",
        "\n",
        "# Process files ONE AT A TIME and save to disk\n",
        "print(\"=\"*80)\n",
        "print(\"EXTRACTING UNIQUE KEYS FROM ALL FILES (ONE AT A TIME)\")\n",
        "print(\"=\"*80)\n",
        "print(\"Results will be saved to disk to avoid memory issues\\n\")\n",
        "\n",
        "cache_dir = Path(\"output/uhc_comparison_cache\")\n",
        "cache_dir.mkdir(exist_ok=True)\n",
        "\n",
        "file_metadata = {}\n",
        "for name, path in existing_files.items():\n",
        "    cache_file = cache_dir / f\"{name.replace(' ', '_')}_keys.pkl\"\n",
        "    \n",
        "    if cache_file.exists():\n",
        "        print(f\"‚úÖ {name}: Loading from cache...\")\n",
        "        with open(cache_file, 'rb') as f:\n",
        "            providers, rate_keys, billing_codes = pickle.load(f)\n",
        "    else:\n",
        "        pq_file = pq.ParquetFile(path)\n",
        "        print(f\"\\n{name}:\")\n",
        "        print(f\"  Rows: {pq_file.metadata.num_rows:,}\")\n",
        "        print(f\"  Row groups: {pq_file.num_row_groups}\")\n",
        "        \n",
        "        providers, rate_keys, billing_codes = extract_unique_keys(pq_file, name)\n",
        "        \n",
        "        # Save to disk immediately\n",
        "        print(f\"  üíæ Saving to cache...\")\n",
        "        with open(cache_file, 'wb') as f:\n",
        "            pickle.dump((providers, rate_keys, billing_codes), f)\n",
        "    \n",
        "    file_metadata[name] = {\n",
        "        'providers_count': len(providers),\n",
        "        'rate_keys_count': len(rate_keys),\n",
        "        'billing_codes_count': len(billing_codes),\n",
        "        'cache_file': cache_file\n",
        "    }\n",
        "    \n",
        "    # Free memory - don't keep sets in memory\n",
        "    del providers, rate_keys, billing_codes\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"‚úÖ EXTRACTION COMPLETE - All results cached to disk\")\n",
        "print(\"=\"*80)\n",
        "for name, meta in file_metadata.items():\n",
        "    print(f\"{name:20s}: {meta['providers_count']:,} providers, {meta['rate_keys_count']:,} rate keys, {meta['billing_codes_count']:,} codes\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "================================================================================\n",
            "RATE KEY OVERLAP MATRIX\n",
            "================================================================================\n",
            "\n",
            "Shows what % of each file's rate keys exist in other files\n",
            "(Loading from cache one pair at a time to avoid memory issues)\n",
            "\n",
            "  Processed 1/7 files...\n",
            "  Processed 2/7 files...\n",
            "  Processed 3/7 files...\n",
            "  Processed 4/7 files...\n",
            "  Processed 5/7 files...\n",
            "  Processed 6/7 files...\n",
            "  Processed 7/7 files...\n",
            "\n",
            "File                 | IL Choice Pl |  IL Core POS | NC Choice Pl | NY Choice EP | NY Choice Pl | GA Charter H | GA Choice HM | \n",
            "-----------------------------------------------------------------------------------------------------------------------------\n",
            "IL Choice Plus       |         100% |         0.3% |         100% |         0.3% |         100% |         0.3% |         0.3% | \n",
            "IL Core POS          |         0.3% |         100% |         0.3% |         0.3% |         0.3% |         0.3% |         0.3% | \n",
            "NC Choice Plus       |        99.5% |         0.3% |         100% |         0.3% |        99.5% |         0.3% |         0.3% | \n",
            "NY Choice EPO        |         0.3% |         0.2% |         0.3% |         100% |         0.3% |         0.3% |         0.3% | \n",
            "NY Choice Plus       |       100.0% |         0.3% |         100% |         0.3% |         100% |         0.3% |         0.3% | \n",
            "GA Charter HMO       |         0.3% |         0.3% |         0.3% |         0.3% |         0.3% |         100% |         0.3% | \n",
            "GA Choice HMO        |         0.3% |         0.2% |         0.3% |         0.3% |         0.3% |         0.3% |         100% | \n"
          ]
        }
      ],
      "source": [
        "# Build comparison matrix for rate keys (load from cache one pair at a time)\n",
        "print(\"=\"*80)\n",
        "print(\"RATE KEY OVERLAP MATRIX\")\n",
        "print(\"=\"*80)\n",
        "print(\"\\nShows what % of each file's rate keys exist in other files\")\n",
        "print(\"(Loading from cache one pair at a time to avoid memory issues)\\n\")\n",
        "\n",
        "file_names = list(file_metadata.keys())\n",
        "matrix = {}\n",
        "\n",
        "# Calculate overlap percentages - load only 2 files at a time\n",
        "for i, file1 in enumerate(file_names):\n",
        "    matrix[file1] = {}\n",
        "    \n",
        "    # Load file1 keys\n",
        "    with open(file_metadata[file1]['cache_file'], 'rb') as f:\n",
        "        _, keys1, _ = pickle.load(f)\n",
        "    \n",
        "    for file2 in file_names:\n",
        "        # Load file2 keys\n",
        "        with open(file_metadata[file2]['cache_file'], 'rb') as f:\n",
        "            _, keys2, _ = pickle.load(f)\n",
        "        \n",
        "        overlap = len(keys1 & keys2)\n",
        "        pct = (overlap / len(keys1) * 100) if len(keys1) > 0 else 0\n",
        "        matrix[file1][file2] = pct\n",
        "        \n",
        "        # Free file2 from memory\n",
        "        del keys2\n",
        "    \n",
        "    # Free file1 from memory\n",
        "    del keys1\n",
        "    print(f\"  Processed {i+1}/{len(file_names)} files...\")\n",
        "\n",
        "# Display matrix\n",
        "print(f\"\\n{'File':<20s} | \", end=\"\")\n",
        "for name in file_names:\n",
        "    print(f\"{name[:12]:>12s} | \", end=\"\")\n",
        "print()\n",
        "print(\"-\" * (20 + 15 * len(file_names)))\n",
        "\n",
        "for file1 in file_names:\n",
        "    print(f\"{file1:<20s} | \", end=\"\")\n",
        "    for file2 in file_names:\n",
        "        pct = matrix[file1][file2]\n",
        "        if pct == 100.0:\n",
        "            print(f\"{'100%':>12s} | \", end=\"\")\n",
        "        elif pct >= 90.0:\n",
        "            print(f\"{pct:>11.1f}% | \", end=\"\")\n",
        "        else:\n",
        "            print(f\"{pct:>11.1f}% | \", end=\"\")\n",
        "    print()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "================================================================================\n",
            "SUBSET RELATIONSHIPS\n",
            "================================================================================\n",
            "\n",
            "Files that are 100% subsets of other files:\n",
            "\n",
            "‚úÖ IL Choice Plus       is 100% subset of NC Choice Plus       (+621,353 extra rate keys)\n",
            "‚úÖ IL Choice Plus       is 100% subset of NY Choice Plus       (+22,457 extra rate keys)\n",
            "‚úÖ NY Choice Plus       is 100% subset of NC Choice Plus       (+598,896 extra rate keys)\n",
            "\n",
            "================================================================================\n",
            "MASTER FILE IDENTIFICATION\n",
            "================================================================================\n",
            "\n",
            "Files ranked by number of unique rate keys:\n",
            "üëë 1. NC Choice Plus      : 119,461,113 unique rate keys\n",
            "   2. GA Choice HMO       : 119,357,919 unique rate keys\n",
            "   3. NY Choice EPO       : 118,958,754 unique rate keys\n",
            "   4. NY Choice Plus      : 118,862,217 unique rate keys\n",
            "   5. IL Choice Plus      : 118,839,760 unique rate keys\n",
            "   6. IL Core POS         : 112,288,723 unique rate keys\n",
            "   7. GA Charter HMO      : 110,864,888 unique rate keys\n"
          ]
        },
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
            "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
            "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
            "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
          ]
        }
      ],
      "source": [
        "# Identify subset relationships (100% overlap = subset)\n",
        "# Use matrix data we already calculated (no need to reload)\n",
        "print(\"=\"*80)\n",
        "print(\"SUBSET RELATIONSHIPS\")\n",
        "print(\"=\"*80)\n",
        "print(\"\\nFiles that are 100% subsets of other files:\\n\")\n",
        "\n",
        "subset_relationships = []\n",
        "for file1 in file_names:\n",
        "    for file2 in file_names:\n",
        "        if file1 != file2:\n",
        "            # Use matrix data (100% means file1 is subset of file2)\n",
        "            if matrix[file1][file2] == 100.0:\n",
        "                extra = file_metadata[file2]['rate_keys_count'] - file_metadata[file1]['rate_keys_count']\n",
        "                subset_relationships.append((file1, file2, extra))\n",
        "\n",
        "if subset_relationships:\n",
        "    for subset, superset, extra in subset_relationships:\n",
        "        print(f\"‚úÖ {subset:20s} is 100% subset of {superset:20s} (+{extra:,} extra rate keys)\")\n",
        "else:\n",
        "    print(\"‚ùå No 100% subset relationships found\")\n",
        "\n",
        "# Find master file (file with most unique rate keys)\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"MASTER FILE IDENTIFICATION\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "file_sizes = [(name, meta['rate_keys_count']) for name, meta in file_metadata.items()]\n",
        "file_sizes.sort(key=lambda x: x[1], reverse=True)\n",
        "\n",
        "print(\"\\nFiles ranked by number of unique rate keys:\")\n",
        "for i, (name, count) in enumerate(file_sizes, 1):\n",
        "    marker = \"üëë\" if i == 1 else \"  \"\n",
        "    print(f\"{marker} {i}. {name:20s}: {count:,} unique rate keys\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "================================================================================\n",
            "UNIQUE DATA PER FILE\n",
            "================================================================================\n",
            "\n",
            "Rate keys that exist ONLY in each file (not in any other):\n",
            "\n",
            "(This may take a few minutes - loading files one at a time)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Calculate unique data per file (estimated from matrix - memory-efficient)\n",
        "print(\"=\"*80)\n",
        "print(\"UNIQUE DATA PER FILE\")\n",
        "print(\"=\"*80)\n",
        "print(\"\\nEstimated unique rate keys per file (using overlap matrix):\\n\")\n",
        "\n",
        "# For each file, find the maximum overlap with any other file\n",
        "# Unique keys ‚âà total keys - max overlap with any other file\n",
        "for name in file_names:\n",
        "    total_keys = file_metadata[name]['rate_keys_count']\n",
        "    \n",
        "    # Find max overlap with any other file\n",
        "    max_overlap_pct = 0\n",
        "    max_overlap_file = None\n",
        "    for other_name in file_names:\n",
        "        if other_name != name:\n",
        "            overlap_pct = matrix[name][other_name]\n",
        "            if overlap_pct > max_overlap_pct:\n",
        "                max_overlap_pct = overlap_pct\n",
        "                max_overlap_file = other_name\n",
        "    \n",
        "    # Estimate unique keys (conservative: total - max overlap)\n",
        "    max_overlap_count = int(total_keys * max_overlap_pct / 100)\n",
        "    estimated_unique = total_keys - max_overlap_count\n",
        "    unique_pct = (estimated_unique / total_keys * 100) if total_keys > 0 else 0\n",
        "    \n",
        "    print(f\"{name:20s}: ~{estimated_unique:,} unique rate keys ({unique_pct:.2f}% of file)\")\n",
        "    if max_overlap_file and max_overlap_pct > 0.1:\n",
        "        print(f\"  {'':20s}  (max overlap: {max_overlap_pct:.1f}% with {max_overlap_file})\")\n",
        "\n",
        "print(\"\\n\" + \"-\"*80)\n",
        "print(\"üí° Note: This is an estimate. True unique keys may be lower if\")\n",
        "print(\"   a key exists in multiple files (not just the max overlap file).\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Provider overlap analysis (using cached metadata only - no loading)\n",
        "print(\"=\"*80)\n",
        "print(\"PROVIDER OVERLAP ANALYSIS\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Find unique providers per file (from metadata)\n",
        "print(\"\\nUnique providers per file:\")\n",
        "provider_counts = {}\n",
        "for name, meta in file_metadata.items():\n",
        "    count = meta['providers_count']\n",
        "    provider_counts[name] = count\n",
        "    print(f\"  {name:20s}: {count:,} providers\")\n",
        "\n",
        "# Find max providers (likely the master)\n",
        "max_providers = max(provider_counts.values())\n",
        "max_provider_file = max(provider_counts.items(), key=lambda x: x[1])[0]\n",
        "\n",
        "print(f\"\\nüìä Maximum providers in single file: {max_providers:,} ({max_provider_file})\")\n",
        "print(f\"   (This is likely the master provider network)\")\n",
        "\n",
        "# Estimate total unique providers (sum - rough estimate)\n",
        "total_estimate = sum(provider_counts.values())\n",
        "print(f\"\\nüí° Rough estimate of total unique providers: ~{total_estimate:,}\")\n",
        "print(f\"   (Actual total may be lower due to overlap)\")\n",
        "print(f\"   (To get exact count, would need to load all provider sets - memory intensive)\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Summary and recommendations (use cached metadata)\n",
        "print(\"=\"*80)\n",
        "print(\"SUMMARY & RECOMMENDATIONS\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Find master file (from metadata)\n",
        "master_name = max(file_metadata.items(), key=lambda x: x[1]['rate_keys_count'])[0]\n",
        "master_meta = file_metadata[master_name]\n",
        "\n",
        "print(f\"\\nüëë MASTER FILE: {master_name}\")\n",
        "print(f\"   Contains {master_meta['rate_keys_count']:,} unique rate keys\")\n",
        "print(f\"   Contains {master_meta['providers_count']:,} unique providers\")\n",
        "print(f\"   Contains {master_meta['billing_codes_count']:,} unique billing codes\")\n",
        "\n",
        "# Count how many files are subsets of master (from matrix)\n",
        "subset_count = 0\n",
        "for name in file_names:\n",
        "    if name != master_name:\n",
        "        if matrix[name][master_name] == 100.0:\n",
        "            subset_count += 1\n",
        "\n",
        "print(f\"\\nüìä {subset_count}/{len(file_metadata)-1} other files are 100% subsets of master\")\n",
        "\n",
        "print(\"\\nüí° INTERPRETATION:\")\n",
        "if subset_count == len(file_metadata) - 1:\n",
        "    print(\"   ‚ö†Ô∏è  All files are subsets of master - likely same national network\")\n",
        "    print(\"   ‚Üí Consider using master file only for analysis\")\n",
        "    print(\"   ‚Üí Other files may differ only in metadata (state/plan labels)\")\n",
        "else:\n",
        "    print(f\"   ‚úÖ {len(file_metadata) - subset_count - 1} files have unique data\")\n",
        "    print(\"   ‚Üí Files represent distinct networks/plans\")\n",
        "    print(\"   ‚Üí All files needed for complete analysis\")\n",
        "\n",
        "print(\"\\nüíæ Cache files saved to: output/uhc_comparison_cache/\")\n",
        "print(\"   (Delete this folder to re-extract from scratch)\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
